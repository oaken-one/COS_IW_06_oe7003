{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TYZMi0LROEfkO3SMhB3T3FCrmecTIVkE",
      "authorship_tag": "ABX9TyPBzJPCWH+Bkm9MqkSHBOZv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaken-one/COS_IW_06_oe7003/blob/main/Independent_Work_Code_Using_Machine_Learning_Models_to_Predict_Discharging_Capacity_in_Lithium_Ion_Batteries_OkezieEze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zsM8IYaSZe4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uploading MATLAB files\n",
        "import scipy.io as sio\n",
        "bat05 = sio.loadmat(\"/content/drive/MyDrive/NASA Battery Data/B0005.ordered.mat\")\n",
        "bat06 = sio.loadmat(\"/content/drive/MyDrive/NASA Battery Data/B0006.ordered.mat\")"
      ],
      "metadata": {
        "id": "Ove8LV6bW-8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Constant Current Charging Voltage\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ccvB0005 = np.zeros([167, 5])\n",
        "ccvB0006 = np.zeros([167, 5])\n",
        "\n",
        "## Find the maximum and minimum values of Constant Current Charging Voltage\n",
        "ccvB0005_max = bat05['B0005'][0,0]['cycle'][0,0]['data'][0,0]['Voltage_measured'].max()\n",
        "ccvB0005_min = bat05['B0005'][0,0]['cycle'][0,0]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "ccvB0006_max = bat06['B0006'][0,0]['cycle'][0,0]['data'][0,0]['Voltage_measured'].max()\n",
        "ccvB0006_min = bat06['B0006'][0,0]['cycle'][0,0]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "for x in range(167):\n",
        "  if ccvB0005_max < bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max():\n",
        "    ccvB0005_max = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max()\n",
        "\n",
        "for x in range(167):\n",
        "  if ccvB0006_max < bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max():\n",
        "    ccvB0006_max = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max()\n",
        "\n",
        "for x in range(167):\n",
        "  if ccvB0005_min > bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min():\n",
        "    ccvB0005_min = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "for x in range(167):\n",
        "  if ccvB0006_min > bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min():\n",
        "    ccvB0006_min = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  ccvB0005[x, 0] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 3] - ccvB0005_min)/(ccvB0005_max - ccvB0005_min)\n",
        "  ccvB0005[x, 1] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 36] - ccvB0005_min)/(ccvB0005_max - ccvB0005_min)\n",
        "  ccvB0005[x, 2] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 69] - ccvB0005_min)/(ccvB0005_max - ccvB0005_min)\n",
        "  ccvB0005[x, 3] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 127] - ccvB0005_min)/(ccvB0005_max - ccvB0005_min)\n",
        "  ccvB0005[x, 4] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 249] - ccvB0005_min)/(ccvB0005_max - ccvB0005_min)\n",
        "\n",
        "for x in range(167):\n",
        "  ccvB0006[x, 0] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 3] - ccvB0006_min)/(ccvB0006_max - ccvB0006_min)\n",
        "  ccvB0006[x, 1] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 36] - ccvB0006_min)/(ccvB0006_max - ccvB0006_min)\n",
        "  ccvB0006[x, 2] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 69] - ccvB0006_min)/(ccvB0006_max - ccvB0006_min)\n",
        "  ccvB0006[x, 3] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 127] - ccvB0006_min)/(ccvB0006_max - ccvB0006_min)\n",
        "  ccvB0006[x, 4] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'][0, 249] - ccvB0006_min)/(ccvB0006_max - ccvB0006_min)"
      ],
      "metadata": {
        "id": "UvykHs2T-t4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Constant Voltage Charging Current\n",
        "cvccB0005 = np.zeros([167, 5])\n",
        "cvccB0006 = np.zeros([167, 5])\n",
        "\n",
        "## Find the maximum and minimum values of Constant Voltage Charging Current\n",
        "cvccB0005_max = bat05['B0005'][0,0]['cycle'][0,0]['data'][0,0]['Current_measured'].max()\n",
        "cvccB0005_min = bat05['B0005'][0,0]['cycle'][0,0]['data'][0,0]['Current_measured'].min()\n",
        "\n",
        "cvccB0006_max = bat06['B0006'][0,0]['cycle'][0,0]['data'][0,0]['Current_measured'].max()\n",
        "cvccB0006_min = bat06['B0006'][0,0]['cycle'][0,0]['data'][0,0]['Current_measured'].min()\n",
        "\n",
        "for x in range(167):\n",
        "  if cvccB0005_max < bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].max():\n",
        "    cvccB0005_max = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].max()\n",
        "\n",
        "for x in range(167):\n",
        "  if cvccB0006_max < bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].max():\n",
        "    cvccB0006_max = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].max()\n",
        "\n",
        "for x in range(167):\n",
        "  if cvccB0005_min > bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].min():\n",
        "    cvccB0005_min = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].min()\n",
        "\n",
        "for x in range(167):\n",
        "  if cvccB0006_min > bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].min():\n",
        "    cvccB0006_min = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'].min()\n",
        "\n",
        "## Choose specific points for training models and normalize them (cycle 32 does not have the same shape as other cycles and must be handled differently)\n",
        "for x in range(167):\n",
        "  if x == 32:\n",
        "    cvccB0005[x, 0] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 4] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 1] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 8] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 2] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 13] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 3] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 28] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 4] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 32] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "  if x != 32:\n",
        "    cvccB0005[x, 0] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 518] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 1] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 535] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 2] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 557] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 3] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 573] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "    cvccB0005[x, 4] = (bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 594] - cvccB0005_min)/(cvccB0005_max - cvccB0005_min)\n",
        "\n",
        "for x in range(167):\n",
        "  if x == 32:\n",
        "    cvccB0006[x, 0] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 4] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 1] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 8] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 2] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 13] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 3] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 28] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 4] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 32] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "  if x != 32:\n",
        "    cvccB0006[x, 0] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 518] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 1] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 535] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 2] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 557] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 3] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 573] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)\n",
        "    cvccB0006[x, 4] = (bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Current_measured'][0, 594] - cvccB0006_min)/(cvccB0006_max - cvccB0006_min)"
      ],
      "metadata": {
        "id": "pY8oKU9CogZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Constant Current Charging Time\n",
        "ccctB0005 = np.zeros([167, 1])\n",
        "ccctB0006 = np.zeros([167, 1])\n",
        "\n",
        "for x in range(167):\n",
        "  max_voltage_index_B5 = np.argmax(bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'])\n",
        "  ccctB0005[x,0] = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Time'][0, max_voltage_index_B5]\n",
        "\n",
        "for x in range(167):\n",
        "  max_voltage_index_B6 = np.argmax(bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'])\n",
        "  ccctB0006[x,0] = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Time'][0, max_voltage_index_B6]\n",
        "\n",
        "## Find the maximum and minimum values of Constant Current Charging Time\n",
        "ccct_B5_max = ccctB0005.max()\n",
        "ccct_B5_min = ccctB0005.min()\n",
        "\n",
        "ccct_B6_max = ccctB0006.max()\n",
        "ccct_B6_min = ccctB0006.min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  ccctB0005[x,0] = (ccctB0005[x,0] - ccct_B5_min)/(ccct_B5_max - ccct_B5_min)\n",
        "  ccctB0006[x,0] = (ccctB0006[x,0] - ccct_B6_min)/(ccct_B6_max - ccct_B6_min)"
      ],
      "metadata": {
        "id": "E3Pf3BsYEdpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Constant Voltage Discharging Current\n",
        "ccdvB0005 = np.zeros([167, 5])\n",
        "ccdvB0006 = np.zeros([167, 5])\n",
        "\n",
        "## Find the maximum and minimum values of Constant Voltage Discharging Current\n",
        "ccdvB0005_max = bat05['B0005'][0,0]['cycle'][0,170]['data'][0,0]['Voltage_measured'].max()\n",
        "ccdvB0005_min = bat05['B0005'][0,0]['cycle'][0,170]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "ccdvB0006_max = bat06['B0006'][0,0]['cycle'][0,170]['data'][0,0]['Voltage_measured'].max()\n",
        "ccdvB0006_min = bat06['B0006'][0,0]['cycle'][0,170]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "for x in range(170, 338):\n",
        "  if ccdvB0005_max < bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max():\n",
        "    ccdvB0005_max = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max()\n",
        "\n",
        "for x in range(170, 338):\n",
        "  if ccdvB0006_max < bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max():\n",
        "    ccdvB0006_max = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].max()\n",
        "\n",
        "for x in range(170, 338):\n",
        "  if ccdvB0005_min > bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min():\n",
        "    ccdvB0005_min = bat05['B0005'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "for x in range(170, 338):\n",
        "  if ccdvB0006_min > bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min():\n",
        "    ccdvB0006_min = bat06['B0006'][0,0]['cycle'][0,x]['data'][0,0]['Voltage_measured'].min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  ccdvB0005[x, 0] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 99] - ccdvB0005_min)/(ccdvB0005_max - ccdvB0005_min)\n",
        "  ccdvB0005[x, 1] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110] - ccdvB0005_min)/(ccdvB0005_max - ccdvB0005_min)\n",
        "  ccdvB0005[x, 2] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115] - ccdvB0005_min)/(ccdvB0005_max - ccdvB0005_min)\n",
        "  ccdvB0005[x, 3] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121] - ccdvB0005_min)/(ccdvB0005_max - ccdvB0005_min)\n",
        "  ccdvB0005[x, 4] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 126] - ccdvB0005_min)/(ccdvB0005_max - ccdvB0005_min)\n",
        "\n",
        "for x in range(167):\n",
        "  ccdvB0006[x, 0] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 99] - ccdvB0006_min)/(ccdvB0006_max - ccdvB0006_min)\n",
        "  ccdvB0006[x, 1] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110] - ccdvB0006_min)/(ccdvB0006_max - ccdvB0006_min)\n",
        "  ccdvB0006[x, 2] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115] - ccdvB0006_min)/(ccdvB0006_max - ccdvB0006_min)\n",
        "  ccdvB0006[x, 3] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121] - ccdvB0006_min)/(ccdvB0006_max - ccdvB0006_min)\n",
        "  ccdvB0006[x, 4] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 126] - ccdvB0006_min)/(ccdvB0006_max - ccdvB0006_min)\n"
      ],
      "metadata": {
        "id": "UOxQdREqL9wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Constant Current Discharging Time\n",
        "ccdtB0005 = np.zeros([167, 1])\n",
        "ccdtB0006 = np.zeros([167, 1])\n",
        "\n",
        "for x in range(167):\n",
        "  max_voltage_index_B5 = np.argmax(bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'])\n",
        "  ccdtB0005[x,0] = bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, max_voltage_index_B5]\n",
        "\n",
        "for x in range(167):\n",
        "  max_voltage_index_B6 = np.argmax(bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'])\n",
        "  ccdtB0006[x,0] = bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, max_voltage_index_B6]\n",
        "\n",
        "## Find the maximum and minimum values of Constant Current Discharging Time\n",
        "ccdt_B5_max = ccdtB0005.max()\n",
        "ccdt_B5_min = ccdtB0005.min()\n",
        "\n",
        "ccdt_B6_max = ccdtB0006.max()\n",
        "ccdt_B6_min = ccdtB0006.min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  ccdtB0005[x,0] = (ccdtB0005[x,0] - ccdt_B5_min)/(ccdt_B5_max - ccdt_B5_min)\n",
        "  ccdtB0006[x,0] = (ccdtB0006[x,0] - ccdt_B6_min)/(ccdt_B6_max - ccdt_B6_min)"
      ],
      "metadata": {
        "id": "hhVriKaPUY4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocessing Rate of Change of Discharging Voltage\n",
        "ccdv_roc_B0005 = np.zeros([167, 4])\n",
        "ccdv_roc_B0006 = np.zeros([167, 4])\n",
        "\n",
        "for x in range(167):\n",
        "  ccdv_roc_B0005[x, 0] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 99])/(bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 110] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 99])\n",
        "  ccdv_roc_B0005[x, 1] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110])/(bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 115] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 110])\n",
        "  ccdv_roc_B0005[x, 2] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115])/(bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 121] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 115])\n",
        "  ccdv_roc_B0005[x, 3] = (bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 126] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121])/(bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 126] - bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 121])\n",
        "\n",
        "  ccdv_roc_B0006[x, 0] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 99])/(bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 110] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 99])\n",
        "  ccdv_roc_B0006[x, 1] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 110])/(bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 115] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 110])\n",
        "  ccdv_roc_B0006[x, 2] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 115])/(bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 121] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 115])\n",
        "  ccdv_roc_B0006[x, 3] = (bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 126] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Voltage_measured'][0, 121])/(bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 126] - bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Time'][0, 121])\n",
        "\n",
        "## Find the maximum and minimum values of Rate of Change of Discharging Voltage\n",
        "ccdv_roc_B0005_max = ccdv_roc_B0005.max()\n",
        "ccdv_roc_B0005_min = ccdv_roc_B0005.min()\n",
        "\n",
        "ccdv_roc_B0006_max = ccdv_roc_B0006.max()\n",
        "ccdv_roc_B0006_min = ccdv_roc_B0006.min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  ccdv_roc_B0005[x, 0] = (ccdv_roc_B0005[x, 0] - ccdv_roc_B0005_min)/(ccdv_roc_B0005_max - ccdv_roc_B0005_min)\n",
        "  ccdv_roc_B0005[x, 1] = (ccdv_roc_B0005[x, 1] - ccdv_roc_B0005_min)/(ccdv_roc_B0005_max - ccdv_roc_B0005_min)\n",
        "  ccdv_roc_B0005[x, 2] = (ccdv_roc_B0005[x, 2] - ccdv_roc_B0005_min)/(ccdv_roc_B0005_max - ccdv_roc_B0005_min)\n",
        "  ccdv_roc_B0005[x, 3] = (ccdv_roc_B0005[x, 3] - ccdv_roc_B0005_min)/(ccdv_roc_B0005_max - ccdv_roc_B0005_min)\n",
        "\n",
        "  ccdv_roc_B0006[x, 0] = (ccdv_roc_B0006[x, 0] - ccdv_roc_B0006_min)/(ccdv_roc_B0006_max - ccdv_roc_B0006_min)\n",
        "  ccdv_roc_B0006[x, 1] = (ccdv_roc_B0006[x, 1] - ccdv_roc_B0006_min)/(ccdv_roc_B0006_max - ccdv_roc_B0006_min)\n",
        "  ccdv_roc_B0006[x, 2] = (ccdv_roc_B0006[x, 2] - ccdv_roc_B0006_min)/(ccdv_roc_B0006_max - ccdv_roc_B0006_min)\n",
        "  ccdv_roc_B0006[x, 3] = (ccdv_roc_B0006[x, 3] - ccdv_roc_B0006_min)/(ccdv_roc_B0006_max - ccdv_roc_B0006_min)"
      ],
      "metadata": {
        "id": "zdv-zDFFWDmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preprocesing Discharging Capacity\n",
        "dcB0005 = np.zeros([167, 1])\n",
        "dcB0006 = np.zeros([167, 1])\n",
        "dcInputB0005 = np.zeros([167, 1])\n",
        "dcInputB0006 = np.zeros([167, 1])\n",
        "\n",
        "for x in range(167):\n",
        "  dcB0005[x, 0] = bat05['B0005'][0,0]['cycle'][0,x + 170]['data'][0,0]['Capacity'][0,0]\n",
        "  dcB0006[x, 0] = bat06['B0006'][0,0]['cycle'][0,x + 170]['data'][0,0]['Capacity'][0,0]\n",
        "\n",
        "## Find the maximum and minimum values of Discharging Capacity\n",
        "dcB0005_max = dcB0005.max()\n",
        "dcB0006_max = dcB0006.max()\n",
        "dcB0005_min = dcB0005.min()\n",
        "dcB0006_min = dcB0006.min()\n",
        "\n",
        "## Choose specific points for training models and normalize them\n",
        "for x in range(167):\n",
        "  dcInputB0005[x, 0] = (dcB0005[x, 0] - dcB0005_min)/(dcB0005_max - dcB0005_min)\n",
        "  dcInputB0006[x, 0] = (dcB0006[x, 0] - dcB0006_min)/(dcB0006_max - dcB0006_min)"
      ],
      "metadata": {
        "id": "JKGyhceAEXT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 1 (Inputs are CCCT and CCV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "jt92IV_jR2ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "VFVAEpNAPiGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "CeJldQkYP0kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 4 (Inputs are Discharging Capacity, CCCT, CCV, and CVCC)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "wMdyUvbthFcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 5 (Inputs are CCDT and CCDV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "GQyJbcHkP6ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 6 (Inputs are CCDV and RoCV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "lbOs6T8tUuF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 7 (Inputs are Discharging Capacity, CCDT, and CCDV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "EVhWZ_9tZBwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear Regression Model with Case 8 (Inputs are Discharging Capacity, CCDT, CCDV, and RoCV)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "JZ6o5htXbDg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 1 (Inputs are CCCT and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "qSy4_V8MkDzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron Model (MLP) with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "cSbP4MPik6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "v7nRmKNJmVJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 4 (Inputs are Discharging Capacity, CCCT, CCV, and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "0gipLtnJn1f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 5 (Inputs are CCDT and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "MW_taR7ApWdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 6 (Inputs are CCDV and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "68pdJaqwp_F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 7 (Inputs are Discharging Capacity, CCDT, and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "U1ND7KGQryA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Multilayer Perceptron (MLP) Model with Case 8 (Inputs are Discharging Capacity, CCDT, CCDV, and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the KFold cross validator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 10\n",
        "for train_index, val_index in kfold.split(X_train_scaled):\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "# MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "2ybcF5X1si4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 1 (Inputs are CCCT and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "sAvL8GL3tHi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "V8EpOG-9xwGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "VCtLBDb1yorj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 4 (Inputs are Discharging Capacity, CCCT, CCV, and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "2A8s7Pi40XUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 5 (Inputs are CCDT and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "eF-7xylz1Wlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 6 (Inputs are CCDV and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "08L8Stqg2NIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 7 (Inputs are Discharging Capacity, CCDT, and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "DLfiNPSQ3Ik2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convolutional Neural Network (CNN) with Case 8 (Inputs are Discharging Capacity, CCDT, CCDV, and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),  # Reduced filter size and kernel size\n",
        "    MaxPooling1D(pool_size=2),  # Standard pooling size\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),  # Slightly less dropout to keep more information\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "JAGfcEO-3kFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 1 (Inputs are CCCT and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "Y03VoBmu5fe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for LSTM input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "2jKJ_PoO8ZRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "q8nrKjwn-SY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 4 (Inputs are Discharging Capacity, CCCT, CCV, and CVCC)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "yzw7TsRcqc2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with with Case 5 (Inputs are CCDT and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "yQWW3FCtyXAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 6 (Inputs are CCDV and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "LN51mkS1I5Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 7 (Inputs are Discharging Capacity, CCDT, and CCDV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "02C3uKSHMetX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Long Short-Term Memory (LSTM) Model with Case 8 (Inputs are Discharging Capacity, CCDT, CCDV, and RoCV)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Flatten inputs for scaling\n",
        "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
        "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape for CNN input\n",
        "X_train_scaled = X_train_scaled_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=10000, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "hm9nmyE2NqHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 1 (Inputs are CCCT and CCV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Combine and reshape data\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "GOtX8dpoVrj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "aArGLj20XtxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "mt4eZllKsU-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 4 (Inputs are Discharging Capacity, CCCT, CCV, and CVCC)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccvB0005, X_train_ccvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccctB0005, X_train_ccctB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_cvccB0005, X_train_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccvB0005, X_test_ccvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccctB0005, X_test_ccctB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_cvccB0005, X_test_cvccB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "Y1zuaGBkt0kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 5 (CCDT and CCDV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "BUygnGv3u16B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 6 (Input are CCDV and RoV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "OPmukUZPvc3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 7 (Discharging Capacity, CCDT, and CCDV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention and normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "\n",
        "    # Feed-forward network and normalization\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape[1]))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)  # Apply Dense to each timestep\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape for model input\n",
        "X_train = X_train.reshape(X_train.shape[0], -1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], -1, X_test.shape[1])\n",
        "\n",
        "# If model outputs at each timestep and you need targets for each timestep:\n",
        "y_train = np.repeat(y_train[:, np.newaxis], X_train.shape[1], axis=1)\n",
        "y_test = np.repeat(y_test[:, np.newaxis], X_test.shape[1], axis=1)\n",
        "\n",
        "# Train the model with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model = build_model((X_train.shape[1], X_train.shape[2]), head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "ELCuYrIAxOfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Transformer Model with Case 8 (Discharging Capacity, CCDT, RoCV, and CCDV)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
        "    return ff\n",
        "\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, dropout=0):\n",
        "    inputs = layers.Input(shape=(None, input_shape))\n",
        "    x = transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Combine training sets from B0005 and B0006\n",
        "X_train = np.concatenate([\n",
        "    np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_train = np.concatenate([y_train_B0005, y_train_B0006], axis=0)\n",
        "\n",
        "# Combine testing sets from B0005 and B0006\n",
        "X_test = np.concatenate([\n",
        "    np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006], axis=0),\n",
        "    np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006], axis=0),\n",
        "    np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006], axis=0)\n",
        "], axis=1)\n",
        "y_test = np.concatenate([y_test_B0005, y_test_B0006], axis=0)\n",
        "\n",
        "# Reshape data if necessary:\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # Add a sequence dimension\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "model = build_model(X_train.shape[2], head_size=128, num_heads=4, ff_dim=512, dropout=0.1)\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=1000, callbacks=[early_stopping])\n",
        "\n",
        "# Prediction and Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test.flatten(), y_pred.flatten())\n",
        "mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())\n",
        "r2 = r2_score(y_test.flatten(), y_pred.flatten())\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "Fdikv00oyIJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba Model with Case 1 (Inputs are CCCT and CCV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "# Assuming bucketing into 1000 categories for each feature\n",
        "num_buckets = 1000\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccv_encoded_train = np.floor(np.concatenate([X_train_ccvB0005, X_train_ccvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_train = np.floor(np.concatenate([X_train_ccctB0005, X_train_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccv_encoded_test = np.floor(np.concatenate([X_test_ccvB0005, X_test_ccvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_test = np.floor(np.concatenate([X_test_ccctB0005, X_test_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([ccv_encoded_train, ccct_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([ccv_encoded_test, ccct_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "# Model adaptation\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        volt_embeddings = self.embedding(features[:, :features.shape[1]//2])\n",
        "        time_embeddings = self.embedding(features[:, features.shape[1]//2:])\n",
        "        combined_embeddings = (volt_embeddings + time_embeddings) / 2\n",
        "        outputs = self.mamba(inputs_embeds=combined_embeddings)\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Building and compiling the model\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # number of epochs\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)  # Directly using test as validation for demonstration\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "5tyovx_OvhpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 2 (Inputs are Discharging Capacity and CVCC)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "# Assuming bucketing into 1000 categories for each feature\n",
        "num_buckets = 1000\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "\n",
        "# Encoding and reshaping\n",
        "dcinput_encoded_train = np.floor(np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_test = np.floor(np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "cvcc_encoded_train = np.floor(np.concatenate([X_train_cvccB0005, X_train_cvccB0006]) * num_buckets).astype(int) % num_buckets\n",
        "cvcc_encoded_test = np.floor(np.concatenate([X_test_cvccB0005, X_test_cvccB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([dcinput_encoded_train, cvcc_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([dcinput_encoded_test, cvcc_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "# Model adaptation\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        volt_embeddings = self.embedding(features[:, :features.shape[1]//2])\n",
        "        time_embeddings = self.embedding(features[:, features.shape[1]//2:])\n",
        "        combined_embeddings = (volt_embeddings + time_embeddings) / 2\n",
        "        outputs = self.mamba(inputs_embeds=combined_embeddings)\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Building and compiling the model\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # number of epochs\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)  # Directly using test as validation for demonstration\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R^2 Score: {r2}\")"
      ],
      "metadata": {
        "id": "jDiDtnyRxMuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 3 (Inputs are Discharging Capacity, CCCT, and CCV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccv_encoded_train = np.floor(np.concatenate([X_train_ccvB0005, X_train_ccvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccv_encoded_test = np.floor(np.concatenate([X_test_ccvB0005, X_test_ccvB0006 ]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_train = np.floor(np.concatenate([X_train_ccctB0005, X_train_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_test = np.floor(np.concatenate([X_test_ccctB0005, X_test_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_train = np.floor(np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_test = np.floor(np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([dcinput_encoded_train, ccv_encoded_train, ccct_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([dcinput_encoded_test, ccv_encoded_test, ccct_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)\n"
      ],
      "metadata": {
        "id": "mPD0lhNS9O_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 4 (Discharging Capacity, CCCT, CCV, and CVC)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccvB0005 = ccvB0005[0:103, :]\n",
        "X_train_ccvB0006 = ccvB0006[0:60, :]\n",
        "X_train_ccctB0005 = ccctB0005[0:103, :]\n",
        "X_train_ccctB0006 = ccctB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_cvccB0005 = cvccB0005[0:103, :]\n",
        "X_train_cvccB0006 = cvccB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccvB0005 = ccvB0005[104:, :]\n",
        "X_test_ccvB0006 = ccvB0006[61:, :]\n",
        "X_test_ccctB0005 = ccctB0005[104:, :]\n",
        "X_test_ccctB0006 = ccctB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_cvccB0005 = cvccB0005[104:, :]\n",
        "X_test_cvccB0006 = cvccB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccv_encoded_train = np.floor(np.concatenate([X_train_ccvB0005, X_train_ccvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccv_encoded_test = np.floor(np.concatenate([X_test_ccvB0005, X_test_ccvB0006 ]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_train = np.floor(np.concatenate([X_train_ccctB0005, X_train_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccct_encoded_test = np.floor(np.concatenate([X_test_ccctB0005, X_test_ccctB0006]) * num_buckets).astype(int) % num_buckets\n",
        "cvcc_encoded_train = np.floor(np.concatenate([X_train_cvccB0005, X_train_cvccB0006]) * num_buckets).astype(int) % num_buckets\n",
        "cvcc_encoded_test = np.floor(np.concatenate([X_test_cvccB0005, X_test_cvccB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_train = np.floor(np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_test = np.floor(np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([dcinput_encoded_train, ccv_encoded_train, ccct_encoded_train, cvcc_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([dcinput_encoded_test, ccv_encoded_test, ccct_encoded_test, cvcc_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "2x-ggOJc9-IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba Model with Case 5 (Inputs are CCDT and CCDV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccdt_encoded_train = np.floor(np.concatenate([X_train_ccdtB0005, X_train_ccdtB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdt_encoded_test = np.floor(np.concatenate([X_test_ccdtB0005, X_test_ccdtB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([ccdt_encoded_train, ccdv_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([ccdt_encoded_test, ccdv_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "R37lFtLD_i5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 6 (Inputs are CCDV and RoCV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccdv_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_RoCV_encoded_train = np.floor(np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_RoCV_encoded_test = np.floor(np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([ccdt_encoded_train, ccdv_RoCV_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([ccdt_encoded_test, ccdv_RoCV_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "OYjU4iX3BSyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 7 (Inputs are Discharging Capacity, CCDT, and CCDV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccdt_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdt_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_train = np.floor(np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_test = np.floor(np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([ccdt_encoded_train, ccdv_encoded_train, dcinput_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([ccdt_encoded_test, ccdv_encoded_test, dcinput_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "ArUqEQQEJ39G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mamba model with Case 8 (Inputs are Discharging Capacity, CCDT, CCDB, and RoCV)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import MambaConfig, MambaModel\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for the Mamba model\n",
        "config = MambaConfig(\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        ")\n",
        "\n",
        "num_buckets = 1000  # Assuming bucketing into 1000 categories for each feature\n",
        "\n",
        "# Prepare the features (X) and the target (y)\n",
        "# Cycles 1 to 103 for B0005 and cycles 1 to 60 for B0006 are considered above 80% SOH condition\n",
        "X_train_ccdtB0005 = ccdtB0005[0:103, :]\n",
        "X_train_ccdtB0006 = ccdtB0006[0:60, :]\n",
        "X_train_ccdvB0005 = ccdvB0005[0:103, :]\n",
        "X_train_ccdvB0006 = ccdvB0006[0:60, :]\n",
        "X_train_dcinputB0005 = dcInputB0005[0:103, :]\n",
        "X_train_dcinputB0006 = dcInputB0006[0:60, :]\n",
        "X_train_ccdv_rocB0005 = ccdv_roc_B0005[0:103, :]\n",
        "X_train_ccdv_rocB0006 = ccdv_roc_B0006[0:60, :]\n",
        "y_train_B0005 = dcB0005[0:103, :]\n",
        "y_train_B0006 = dcB0006[0:60, :]\n",
        "\n",
        "# Cycles 104 to 167 for B0005 and cycles 61 to 167 for B0006 are considered under 80% SOH condition\n",
        "X_test_ccdtB0005 = ccdtB0005[104:, :]\n",
        "X_test_ccdtB0006 = ccdtB0006[61:, :]\n",
        "X_test_ccdvB0005 = ccdvB0005[104:, :]\n",
        "X_test_ccdvB0006 = ccdvB0006[61:, :]\n",
        "X_test_dcinputB0005 = dcInputB0005[104:, :]\n",
        "X_test_dcinputB0006 = dcInputB0006[61:, :]\n",
        "X_test_ccdv_rocB0005 = ccdv_roc_B0005[104:, :]\n",
        "X_test_ccdv_rocB0006 = ccdv_roc_B0006[61:, :]\n",
        "y_test_B0005 = dcB0005[104:, :]\n",
        "y_test_B0006 = dcB0006[61:, :]\n",
        "\n",
        "# Encoding and reshaping\n",
        "ccdt_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdt_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_train = np.floor(np.concatenate([X_train_ccdvB0005, X_train_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_encoded_test = np.floor(np.concatenate([X_test_ccdvB0005, X_test_ccdvB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_roc_encoded_train = np.floor(np.concatenate([X_train_ccdv_rocB0005, X_train_ccdv_rocB0006]) * num_buckets).astype(int) % num_buckets\n",
        "ccdv_roc_encoded_test = np.floor(np.concatenate([X_test_ccdv_rocB0005, X_test_ccdv_rocB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_train = np.floor(np.concatenate([X_train_dcinputB0005, X_train_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "dcinput_encoded_test = np.floor(np.concatenate([X_test_dcinputB0005, X_test_dcinputB0006]) * num_buckets).astype(int) % num_buckets\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(np.concatenate([ccdt_encoded_train, ccdv_encoded_train, dcinput_encoded_train, ccdv_roc_encoded_train], axis=1), dtype=torch.long)\n",
        "y_train = torch.tensor(np.concatenate([y_train_B0005, y_train_B0006]), dtype=torch.float32)\n",
        "X_test = torch.tensor(np.concatenate([ccdt_encoded_test, ccdv_encoded_test, dcinput_encoded_test, ccdv_roc_encoded_test], axis=1), dtype=torch.long)\n",
        "y_test = torch.tensor(np.concatenate([y_test_B0005, y_test_B0006]), dtype=torch.float32)\n",
        "\n",
        "class RegressionMamba(nn.Module):\n",
        "    def __init__(self, config, num_buckets):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_buckets, config.hidden_size)\n",
        "        self.mamba = MambaModel(config)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, features):\n",
        "        embeddings = self.embedding(features)\n",
        "        # Aggregate embeddings: average across the sequence length (here across all features)\n",
        "        pooled_embeddings = embeddings.mean(dim=1)\n",
        "        outputs = self.mamba(inputs_embeds=pooled_embeddings.unsqueeze(1))\n",
        "        regression_output = self.regressor(outputs.last_hidden_state[:, -1])\n",
        "        return regression_output.squeeze()\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = RegressionMamba(config, num_buckets)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(X_train)\n",
        "    loss = loss_fn(predictions, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X)\n",
        "        loss = loss_fn(predictions, y)\n",
        "        return loss, predictions\n",
        "\n",
        "# Training and evaluation code setup\n",
        "for epoch in range(50):\n",
        "    train_loss = train()\n",
        "    val_loss, _ = evaluate(X_test, y_test)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {val_loss}\")\n",
        "\n",
        "# Testing and metric evaluation\n",
        "test_loss, predictions = evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# Calculate MSE, MAE, and R2 score\n",
        "predictions_np = predictions.detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "mse = mean_squared_error(y_test_np, predictions_np)\n",
        "mae = mean_absolute_error(y_test_np, predictions_np)\n",
        "r2 = r2_score(y_test_np, predictions_np)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R^2:\", r2)"
      ],
      "metadata": {
        "id": "7ZrcKObzRG7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}